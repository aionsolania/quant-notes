## Information

Information quantifies uncertainty reduction in random
variables.

All information measures are defined relative to an
underlying probability distribution.

---

## Entropy

The entropy of a discrete random variable $X$ is defined as

$$
H(X) = -\sum_x p(x)\log p(x)
$$

Entropy measures the irreducible uncertainty in $X$.

---

## Relative Entropy

The relative entropy (Kullbackâ€“Leibler divergence) between
distributions $P$ and $Q$ is defined as

$$
D_{KL}(P \| Q) = \mathbb{E}_P \left[ \log \\frac{P}{Q} \\right]
$$

It quantifies model mismatch rather than distance.

---

## Modeling Perspective

Information-theoretic quantities expose assumptions embedded
in statistical and optimization models.

